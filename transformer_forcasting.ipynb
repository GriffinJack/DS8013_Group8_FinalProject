{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "start = '2010-01-01'\n",
    "end = '2023-03-01'\n",
    "\n",
    "gold_stock = yf.download('GC=F', start=start, end=end, interval='1d', auto_adjust=True)\n",
    "gold_stock.to_csv('gold_stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target-3</th>\n",
       "      <th>Target-2</th>\n",
       "      <th>Target-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1133.099976</td>\n",
       "      <td>1117.699951</td>\n",
       "      <td>1118.099976</td>\n",
       "      <td>1135.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>1138.199951</td>\n",
       "      <td>1118.099976</td>\n",
       "      <td>1135.900024</td>\n",
       "      <td>1133.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>1150.699951</td>\n",
       "      <td>1135.900024</td>\n",
       "      <td>1133.099976</td>\n",
       "      <td>1138.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>1128.900024</td>\n",
       "      <td>1133.099976</td>\n",
       "      <td>1138.199951</td>\n",
       "      <td>1150.699951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>1136.400024</td>\n",
       "      <td>1138.199951</td>\n",
       "      <td>1150.699951</td>\n",
       "      <td>1128.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3302</th>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>1832.000000</td>\n",
       "      <td>1842.000000</td>\n",
       "      <td>1840.400024</td>\n",
       "      <td>1833.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>1818.000000</td>\n",
       "      <td>1840.400024</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1832.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3304</th>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>1808.800049</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1832.000000</td>\n",
       "      <td>1818.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305</th>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>1817.000000</td>\n",
       "      <td>1832.000000</td>\n",
       "      <td>1818.000000</td>\n",
       "      <td>1808.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>1828.900024</td>\n",
       "      <td>1818.000000</td>\n",
       "      <td>1808.800049</td>\n",
       "      <td>1817.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3307 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Target     Target-3     Target-2     Target-1\n",
       "0     2010-01-07  1133.099976  1117.699951  1118.099976  1135.900024\n",
       "1     2010-01-08  1138.199951  1118.099976  1135.900024  1133.099976\n",
       "2     2010-01-11  1150.699951  1135.900024  1133.099976  1138.199951\n",
       "3     2010-01-12  1128.900024  1133.099976  1138.199951  1150.699951\n",
       "4     2010-01-13  1136.400024  1138.199951  1150.699951  1128.900024\n",
       "...          ...          ...          ...          ...          ...\n",
       "3302  2023-02-22  1832.000000  1842.000000  1840.400024  1833.000000\n",
       "3303  2023-02-23  1818.000000  1840.400024  1833.000000  1832.000000\n",
       "3304  2023-02-24  1808.800049  1833.000000  1832.000000  1818.000000\n",
       "3305  2023-02-27  1817.000000  1832.000000  1818.000000  1808.800049\n",
       "3306  2023-02-28  1828.900024  1818.000000  1808.800049  1817.000000\n",
       "\n",
       "[3307 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_stock = pd.read_csv(\"transformed_data.csv\")\n",
    "gold_stock.drop(columns=gold_stock.columns[0], axis=1, inplace=True)\n",
    "gold_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_split_columns(df):\n",
    "    #convert to numpy array\n",
    "    df = df.to_numpy()\n",
    "\n",
    "    dates = df[:,0]\n",
    "\n",
    "    Y = df[:,1]\n",
    "\n",
    "    X = df[:, 2:]\n",
    "    X = X.reshape((len(dates), X.shape[1], 1))\n",
    "\n",
    "    return dates, X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "dates, X, Y = window_split_columns(gold_stock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent80 = int(len(dates)*.8)\n",
    "percent90 = int(len(dates)*.9)\n",
    "\n",
    "dates_train, X_train, y_train = dates[:percent80], X[:percent80], Y[:percent80]\n",
    "dates_val, X_val, y_val = dates[percent80:percent90], X[percent80:percent90], Y[percent80:percent90]\n",
    "dates_test, X_test, y_test = dates[percent90:], X[percent90:], Y[percent90:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 2s 12ms/step - loss: 1893735.2500 - val_loss: 3274475.5000\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1892981.5000 - val_loss: 3269586.2500\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1892207.3750 - val_loss: 3266257.2500\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1891354.7500 - val_loss: 3263406.7500\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1890389.0000 - val_loss: 3260359.5000\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1889237.0000 - val_loss: 3256803.5000\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1887915.7500 - val_loss: 3252418.2500\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1886335.3750 - val_loss: 3247199.2500\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1884517.0000 - val_loss: 3240926.0000\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1882341.7500 - val_loss: 3233531.0000\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1879977.8750 - val_loss: 3224939.2500\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1877087.3750 - val_loss: 3214962.7500\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1873928.5000 - val_loss: 3203684.7500\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1870439.7500 - val_loss: 3190882.5000\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1866387.1250 - val_loss: 3176343.5000\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1862041.0000 - val_loss: 3160281.5000\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1857252.2500 - val_loss: 3142282.0000\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1852032.5000 - val_loss: 3122129.5000\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1846427.6250 - val_loss: 3100089.0000\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1840201.5000 - val_loss: 3076127.0000\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1833729.8750 - val_loss: 3049596.2500\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1826660.5000 - val_loss: 3020892.5000\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1818336.8750 - val_loss: 2989955.2500\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1810496.1250 - val_loss: 2957109.2500\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1801287.3750 - val_loss: 2922295.0000\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1792237.7500 - val_loss: 2885442.0000\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1782011.5000 - val_loss: 2845644.5000\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1771465.3750 - val_loss: 2804481.7500\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1760956.3750 - val_loss: 2761290.7500\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1749071.7500 - val_loss: 2716207.2500\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1737419.2500 - val_loss: 2668732.0000\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1725066.3750 - val_loss: 2619337.7500\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1711103.3750 - val_loss: 2568742.7500\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1697261.7500 - val_loss: 2515057.0000\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1684284.1250 - val_loss: 2461913.0000\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1669882.8750 - val_loss: 2406536.0000\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1654318.1250 - val_loss: 2349237.2500\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1639644.6250 - val_loss: 2291807.7500\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1622730.6250 - val_loss: 2232061.5000\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1608740.2500 - val_loss: 2170845.7500\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1589402.7500 - val_loss: 2110320.2500\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1572913.6250 - val_loss: 2047026.1250\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 1555394.5000 - val_loss: 1983738.2500\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1538735.5000 - val_loss: 1918651.0000\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1519328.5000 - val_loss: 1854452.6250\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1501220.0000 - val_loss: 1787908.2500\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1478701.1250 - val_loss: 1716418.3750\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1459978.8750 - val_loss: 1646749.0000\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1438821.3750 - val_loss: 1578679.2500\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1417235.5000 - val_loss: 1511077.5000\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1397145.2500 - val_loss: 1442398.8750\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1374693.6250 - val_loss: 1375055.1250\n",
      "Epoch 53/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1353630.5000 - val_loss: 1309350.3750\n",
      "Epoch 54/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1329447.6250 - val_loss: 1240646.6250\n",
      "Epoch 55/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1308068.3750 - val_loss: 1175387.1250\n",
      "Epoch 56/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1284186.0000 - val_loss: 1111250.0000\n",
      "Epoch 57/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1261750.6250 - val_loss: 1047984.5625\n",
      "Epoch 58/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1239386.7500 - val_loss: 986715.7500\n",
      "Epoch 59/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1212804.6250 - val_loss: 927902.6250\n",
      "Epoch 60/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1189164.0000 - val_loss: 873554.0625\n",
      "Epoch 61/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1167543.0000 - val_loss: 822563.9375\n",
      "Epoch 62/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1141304.3750 - val_loss: 773128.6250\n",
      "Epoch 63/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1116674.5000 - val_loss: 728193.1875\n",
      "Epoch 64/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1089011.3750 - val_loss: 685436.3125\n",
      "Epoch 65/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1064167.0000 - val_loss: 647943.1250\n",
      "Epoch 66/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1039033.6250 - val_loss: 612231.4375\n",
      "Epoch 67/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1010948.1875 - val_loss: 578580.1875\n",
      "Epoch 68/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 981707.2500 - val_loss: 547191.5625\n",
      "Epoch 69/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 949784.6875 - val_loss: 517066.6875\n",
      "Epoch 70/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 922083.7500 - val_loss: 489199.3750\n",
      "Epoch 71/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 893151.6875 - val_loss: 462973.0000\n",
      "Epoch 72/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 864388.1250 - val_loss: 439002.0625\n",
      "Epoch 73/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 832865.2500 - val_loss: 412820.7500\n",
      "Epoch 74/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 802223.5000 - val_loss: 385086.5000\n",
      "Epoch 75/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 770554.4375 - val_loss: 358692.9375\n",
      "Epoch 76/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 740059.8750 - val_loss: 334572.1875\n",
      "Epoch 77/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 707822.7500 - val_loss: 311652.5312\n",
      "Epoch 78/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 676675.4375 - val_loss: 290400.0625\n",
      "Epoch 79/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 648745.3750 - val_loss: 269255.6562\n",
      "Epoch 80/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 616845.8750 - val_loss: 250343.5625\n",
      "Epoch 81/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 590742.0000 - val_loss: 232479.8281\n",
      "Epoch 82/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 561464.3125 - val_loss: 214908.0156\n",
      "Epoch 83/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 534086.8125 - val_loss: 197423.8281\n",
      "Epoch 84/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 507638.7188 - val_loss: 179972.2812\n",
      "Epoch 85/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 482252.9375 - val_loss: 163704.9844\n",
      "Epoch 86/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 454676.2188 - val_loss: 150063.0781\n",
      "Epoch 87/100\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 432112.7188 - val_loss: 138200.0781\n",
      "Epoch 88/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 410839.0000 - val_loss: 126184.6406\n",
      "Epoch 89/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 385485.3438 - val_loss: 116335.5391\n",
      "Epoch 90/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 363943.5000 - val_loss: 107277.4609\n",
      "Epoch 91/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 343610.6250 - val_loss: 98684.2031\n",
      "Epoch 92/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 322143.2812 - val_loss: 90157.1406\n",
      "Epoch 93/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 305904.8750 - val_loss: 83198.3359\n",
      "Epoch 94/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 289058.7812 - val_loss: 77059.3516\n",
      "Epoch 95/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 271885.1875 - val_loss: 71840.0938\n",
      "Epoch 96/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 257804.5625 - val_loss: 66705.1328\n",
      "Epoch 97/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 240799.5312 - val_loss: 61466.8711\n",
      "Epoch 98/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 226947.1250 - val_loss: 57533.8086\n",
      "Epoch 99/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 216941.8281 - val_loss: 54072.1641\n",
      "Epoch 100/100\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 203294.7344 - val_loss: 51418.8828\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 43477.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43477.9921875"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import transformer_model\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "def transformer_model(input_dim, output_dim, num_heads, ff_dim, dropout_rate, max_len):\n",
    "    inputs = keras.layers.Input(shape=(max_len, input_dim))\n",
    "    x = keras.layers.BatchNormalization()(inputs)\n",
    "    x = keras.layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = keras.layers.Dense(output_dim, activation=\"linear\")(x)\n",
    "    x = keras.layers.Transformer()\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Set hyperparameters\n",
    "input_dim = 1 # Number of input features\n",
    "output_dim = 1 # Number of output features\n",
    "num_heads = 2 # Number of attention heads\n",
    "ff_dim = 32 # Dimensionality of the feedforward network\n",
    "dropout_rate = 0.2 # Dropout rate\n",
    "max_len = 3 # Maximum length of input sequence\n",
    "\n",
    "# Initialize the model\n",
    "model = transformer_model(input_dim, output_dim, num_heads, ff_dim, dropout_rate, max_len)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
